## PyTorch for Deep Learning Bootcamp
- Instructor:
  - Andrei Neagoie
  - Daniel Bourke

## Section 1: Introduction

### 1. PyTorch for Deep Learning

### 2. Course Welcome and What Is Deep Learning

### 3. Join Our Online Classroom!

### 4. Exercise: Meet Your Classmates + Instructor

### 5. Free Course Book + Code Resources + Asking Questions + Getting Help
- https://github.com/mrdbourke/pytorch-deep-learning

### 6. ZTM Resources

### 7. Machine Learning + Python Monthly Newsletters

## Section 2: PyTorch Fundamentals

### 8. Why Use Machine Learning or Deep Learning

### 9. The Number 1 Rule of Machine Learning and What Is Deep Learning Good For
- What deep learning is good for
  - Problems with long lists of rules
  - Continually changing environments
  - Discovering insights within large collections of data
- What deep learning is not good for
  - When you need explainability
  - When the traditional approach is a better option
  - When errors are unacceptable
  - When you don't have much data

### 10. Machine Learning vs. Deep Learning
- ML
  - For Structured data
- DL
  - For unstructured data

### 11. Anatomy of Neural Networks
- Input layler
- Hidden layer
- Output layer
- Each layer is usually combination of linear and/or nonlinear functions

### 12. Different Types of Learning Paradigms
- Supervised learning
- Unsupervised & self-supervised learning
- Transfer learning

### 13. What Can Deep Learning Be Used For
- Recommendation
- Translation: seq2seq
- Speech recognition: seq2seq
- Computer vision: classification
- Natural Language Processing: classification/regression

### 14. What Is and Why PyTorch
- The most popular research deep learning framework
- Write fast DL code in Python
- Able to access many pre-built DL models
- Whole stack: preprocess data, model data, deploy model in application/cloud
 
### 15. What Are Tensors

### 16. What We Are Going To Cover With PyTorch

### 17. How To and How Not To Approach This Course

### 18. Important Resources For This Course

### 19. Getting Setup to Write PyTorch Code

### 20. Introduction to PyTorch Tensors

### 21. Creating Random Tensors in PyTorch

### 22. Creating Tensors With Zeros and Ones in PyTorch

### 23. Creating a Tensor Range and Tensors Like Other Tensors

### 24. Dealing With Tensor Data Types

### 25. Getting Tensor Attributes

### 26. Manipulating Tensors (Tensor Operations)

### 27. Matrix Multiplication (Part 1)

### 28. Matrix Multiplication (Part 2): The Two Main Rules of Matrix Multiplication

### 29. Matrix Multiplication (Part 3): Dealing With Tensor Shape Errors

### 30. Finding the Min Max Mean and Sum of Tensors (Tensor Aggregation)

### 31. Finding The Positional Min and Max of Tensors

### 32. Reshaping, Viewing and Stacking Tensors

### 33. Squeezing, Unsqueezing and Permuting Tensors

### 34. Selecting Data From Tensors (Indexing)

### 35. PyTorch Tensors and NumPy

### 36. PyTorch Reproducibility (Taking the Random Out of Random)

### 37. Different Ways of Accessing a GPU in PyTorch

### 38. Setting up Device-Agnostic Code and Putting Tensors On and Off the GPU

### 39. PyTorch Fundamentals: Exercises and Extra-Curriculum

    5min
### 40. Introduction and Where You Can Get Help
### 41. Getting Setup and What We Are Covering
### 42. Creating a Simple Dataset Using the Linear Regression Formula
### 43. Splitting Our Data Into Training and Test Sets
### 44. Building a function to Visualize Our Data
### 45. Creating Our First PyTorch Model for Linear Regression
### 46. Breaking Down What's Happening in Our PyTorch Linear regression Model
### 47. Discussing Some of the Most Important PyTorch Model Building Classes
### 48. Checking Out the Internals of Our PyTorch Model
### 49. Making Predictions With Our Random Model Using Inference Mode
### 50. Training a Model Intuition (The Things We Need)
### 51. Setting Up an Optimizer and a Loss Function
### 52. PyTorch Training Loop Steps and Intuition
### 53. Writing Code for a PyTorch Training Loop
### 54. Reviewing the Steps in a Training Loop Step by Step
### 55. Running Our Training Loop Epoch by Epoch and Seeing What Happens
### 56. Writing Testing Loop Code and Discussing What's Happening Step by Step
### 57. Reviewing What Happens in a Testing Loop Step by Step
### 58. Writing Code to Save a PyTorch Model
### 59. Writing Code to Load a PyTorch Model
### 60. Setting Up to Practice Everything We Have Done Using Device Agnostic code
### 61. Putting Everything Together (Part 1): Data
### 62. Putting Everything Together (Part 2): Building a Model
### 63. Putting Everything Together (Part 3): Training a Model
### 64. Putting Everything Together (Part 4): Making Predictions With a Trained Model
### 65. Putting Everything Together (Part 5): Saving and Loading a Trained Model
### 66. Exercise: Imposter Syndrome
### 67. PyTorch Workflow: Exercises and Extra-Curriculum

    4min
### 68. Introduction to Machine Learning Classification With PyTorch
### 69. Classification Problem Example: Input and Output Shapes
### 70. Typical Architecture of a Classification Neural Network (Overview)
### 71. Making a Toy Classification Dataset
### 72. Turning Our Data into Tensors and Making a Training and Test Split
### 73. Laying Out Steps for Modelling and Setting Up Device-Agnostic Code
### 74. Coding a Small Neural Network to Handle Our Classification Data
### 75. Making Our Neural Network Visual
### 76. Recreating and Exploring the Insides of Our Model Using nn.Sequential
### 77. Loss Function Optimizer and Evaluation Function for Our Classification Network
### 78. Going from Model Logits to Prediction Probabilities to Prediction Labels
### 79. Coding a Training and Testing Optimization Loop for Our Classification Model
### 80. Writing Code to Download a Helper Function to Visualize Our Models Predictions
### 81. Discussing Options to Improve a Model
### 82. Creating a New Model with More Layers and Hidden Units
### 83. Writing Training and Testing Code to See if Our Upgraded Model Performs Better
### 84. Creating a Straight Line Dataset to See if Our Model is Learning Anything
### 85. Building and Training a Model to Fit on Straight Line Data
### 86. Evaluating Our Models Predictions on Straight Line Data
### 87. Introducing the Missing Piece for Our Classification Model Non-Linearity
### 88. Building Our First Neural Network with Non-Linearity
### 89. Writing Training and Testing Code for Our First Non-Linear Model
### 90. Making Predictions with and Evaluating Our First Non-Linear Model
### 91. Replicating Non-Linear Activation Functions with Pure PyTorch
### 92. Putting It All Together (Part 1): Building a Multiclass Dataset
### 93. Creating a Multi-Class Classification Model with PyTorch
### 94. Setting Up a Loss Function and Optimizer for Our Multi-Class Model
### 95. Logits to Prediction Probabilities to Prediction Labels with a Multi-Class Model
### 96. Training a Multi-Class Classification Model and Troubleshooting Code on the Fly
### 97. Making Predictions with and Evaluating Our Multi-Class Classification Model
### 98. Discussing a Few More Classification Metrics
### 99. PyTorch Classification: Exercises and Extra-Curriculum

    3min
### 100. What Is a Computer Vision Problem and What We Are Going to Cover
### 101. Computer Vision Input and Output Shapes
### 102. What Is a Convolutional Neural Network (CNN)
### 103. Discussing and Importing the Base Computer Vision Libraries in PyTorch
### 104. Getting a Computer Vision Dataset and Checking Out Its- Input and Output Shapes
### 105. Visualizing Random Samples of Data
### 106. DataLoader Overview Understanding Mini-Batches
### 107. Turning Our Datasets Into DataLoaders
### 108. Model 0: Creating a Baseline Model with Two Linear Layers
### 109. Creating a Loss Function: an Optimizer for Model 0
### 110. Creating a Function to Time Our Modelling Code
### 111. Writing Training and Testing Loops for Our Batched Data
### 112. Writing an Evaluation Function to Get Our Models Results
### 113. Setup Device-Agnostic Code for Running Experiments on the GPU
### 114. Model 1: Creating a Model with Non-Linear Functions
### 115. Mode 1: Creating a Loss Function and Optimizer
### 116. Turing Our Training Loop into a Function
### 117. Turing Our Testing Loop into a Function
### 118. Training and Testing Model 1 with Our Training and Testing Functions
### 119. Getting a Results Dictionary for Model 1
### 120. Model 2: Convolutional Neural Networks High Level Overview
### 121. Model 2: Coding Our First Convolutional Neural Network with PyTorch
### 122. Model 2: Breaking Down Conv2D Step by Step
### 123. Model 2: Breaking Down MaxPool2D Step by Step
### 124. Mode 2: Using a Trick to Find the Input and Output Shapes of Each of Our Layers
### 125. Model 2: Setting Up a Loss Function and Optimizer
### 126. Model 2: Training Our First CNN and Evaluating Its Results
### 127. Comparing the Results of Our Modelling Experiments
### 128. Making Predictions on Random Test Samples with the Best Trained Model
### 129. Plotting Our Best Model Predictions on Random Test Samples and Evaluating Them
### 130. Making Predictions and Importing Libraries to Plot a Confusion Matrix
### 131. Evaluating Our Best Models Predictions with a Confusion Matrix
### 132. Saving and Loading Our Best Performing Model
### 133. Recapping What We Have Covered Plus Exercises and Extra-Curriculum

    6min
### 134. What Is a Custom Dataset and What We Are Going to Cover
### 135. Importing PyTorch and Setting Up Device Agnostic Code
### 136. Downloading a Custom Dataset of Pizza, Steak and Sushi Images
### 137. Becoming One With the Data (Part 1): Exploring the Data Format
### 138. Becoming One With the Data (Part 2): Visualizing a Random Image
### 139. Becoming One With the Data (Part 3): Visualizing a Random Image with Matplotlib
### 140. Transforming Data (Part 1): Turning Images Into Tensors
### 141. Transforming Data (Part 2): Visualizing Transformed Images
### 142. Loading All of Our Images and Turning Them Into Tensors With ImageFolder
### 143. Visualizing a Loaded Image From the Train Dataset
### 144. Turning Our Image Datasets into PyTorch Dataloaders
### 145. Creating a Custom Dataset Class in PyTorch High Level Overview
### 146. Creating a Helper Function to Get Class Names From a Directory
### 147. Writing a PyTorch Custom Dataset Class from Scratch to Load Our Images
### 148. Compare Our Custom Dataset Class. to the Original Imagefolder Class
### 149. Writing a Helper Function to Visualize Random Images from Our Custom Dataset
### 150. Turning Our Custom Datasets Into DataLoaders
### 151. Exploring State of the Art Data Augmentation With Torchvision Transforms
### 152. Building a Baseline Model (Part 1): Loading and Transforming Data
### 153. Building a Baseline Model (Part 2): Replicating Tiny VGG from Scratch
### 154. Building a Baseline Model (Part 3):Doing a Forward Pass to Test Our Model Shapes
### 155. Using the Torchinfo Package to Get a Summary of Our Model
### 156. Creating Training and Testing loop Functions
### 157. Creating a Train Function to Train and Evaluate Our Models
### 158. Training and Evaluating Model 0 With Our Training Functions
### 159. Plotting the Loss Curves of Model 0
### 160. The Balance Between Overfitting and Underfitting and How to Deal With Each
### 161. Creating Augmented Training Datasets and DataLoaders for Model 1
### 162. Constructing and Training Model 1
### 163. Plotting the Loss Curves of Model 1
### 164. Plotting the Loss Curves of All of Our Models Against Each Other
### 165. Predicting on Custom Data (Part 1): Downloading an Image
### 166. Predicting on Custom Data (Part 2): Loading In a Custom Image With PyTorch
### 167. Predicting on Custom Data (Part3):Getting Our Custom Image Into the Right Format
### 168. Predicting on Custom Data (Part4):Turning Our Models Raw Outputs Into Prediction
### 169. Predicting on Custom Data (Part 5): Putting It All Together
### 170. Summary of What We Have Covered Plus Exercises and Extra-Curriculum

    6min
### 171. What Is Going Modular and What We Are Going to Cover
### 172. Going Modular Notebook (Part 1): Running It End to End
### 173. Downloading a Dataset
### 174. Writing the Outline for Our First Python Script to Setup the Data
### 175. Creating a Python Script to Create Our PyTorch DataLoaders
### 176. Turning Our Model Building Code into a Python Script
### 177. Turning Our Model Training Code into a Python Script
### 178. Turning Our Utility Function to Save a Model into a Python Script
### 179. Creating a Training Script to Train Our Model in One Line of Code
### 180. Going Modular: Summary, Exercises and Extra-Curriculum

    6min
### 181. Introduction: What is Transfer Learning and Why Use It
### 182. Where Can You Find Pretrained Models and What We Are Going to Cover
### 183. Installing the Latest Versions of Torch and Torchvision
### 184. Downloading Our Previously Written Code from Going Modular
### 185. Downloading Pizza, Steak, Sushi Image Data from Github
### 186. Turning Our Data into DataLoaders with Manually Created Transforms
### 187. Turning Our Data into DataLoaders with Automatic Created Transforms
### 188. Which Pretrained Model Should You Use
### 189. Setting Up a Pretrained Model with Torchvision
### 190. Different Kinds of Transfer Learning
### 191. Getting a Summary of the Different Layers of Our Model
### 192. Freezing the Base Layers of Our Model and Updating the Classifier Head
### 193. Training Our First Transfer Learning Feature Extractor Model
### 194. Plotting the Loss curves of Our Transfer Learning Model
### 195. Outlining the Steps to Make Predictions on the Test Images
### 196. Creating a Function Predict On and Plot Images
### 197. Making and Plotting Predictions on Test Images
### 198. Making a Prediction on a Custom Image
### 199. Main Takeaways, Exercises and Extra- Curriculum

    3min
### 200. What Is Experiment Tracking and Why Track Experiments
### 201. Getting Setup by Importing Torch Libraries and Going Modular Code
### 202. Creating a Function to Download Data
### 203. Turning Our Data into DataLoaders Using Manual Transforms
### 204. Turning Our Data into DataLoaders Using Automatic Transforms
### 205. Preparing a Pretrained Model for Our Own Problem
### 206. Setting Up a Way to Track a Single Model Experiment with TensorBoard
### 207. Training a Single Model and Saving the Results to TensorBoard
### 208. Exploring Our Single Models Results with TensorBoard
### 209. Creating a Function to Create SummaryWriter Instances
### 210. Adapting Our Train Function to Be Able to Track Multiple Experiments
### 211. What Experiments Should You Try
### 212. Discussing the Experiments We Are Going to Try
### 213. Downloading Datasets for Our Modelling Experiments
### 214. Turning Our Datasets into DataLoaders Ready for Experimentation
### 215. Creating Functions to Prepare Our Feature Extractor Models
### 216. Coding Out the Steps to Run a Series of Modelling Experiments
### 217. Running Eight Different Modelling Experiments in 5 Minutes
### 218. Viewing Our Modelling Experiments in TensorBoard
### 219. Loading the Best Model and Making Predictions on Random Images from the Test Set
### 220. Making a Prediction on Our Own Custom Image with the Best Model
### 221. Main Takeaways, Exercises and Extra- Curriculum

    4min
### 222. What Is a Machine Learning Research Paper?
### 223. Why Replicate a Machine Learning Research Paper?
### 224. Where Can You Find Machine Learning Research Papers and Code?
### 225. What We Are Going to Cover
### 226. Getting Setup for Coding in Google Colab
### 227. Downloading Data for Food Vision Mini
### 228. Turning Our Food Vision Mini Images into PyTorch DataLoaders
### 229. Visualizing a Single Image
### 230. Replicating a Vision Transformer - High Level Overview
### 231. Breaking Down Figure 1 of the ViT Paper
### 232. Breaking Down the Four Equations Overview and a Trick for Reading Papers
### 233. Breaking Down Equation 1
### 234. Breaking Down Equation 2 and 3
### 235. Breaking Down Equation 4
### 236. Breaking Down Table 1
### 237. Calculating the Input and Output Shape of the Embedding Layer by Hand
### 238. Turning a Single Image into Patches (Part 1: Patching the Top Row)
### 239. Turning a Single Image into Patches (Part 2: Patching the Entire Image)
### 240. Creating Patch Embeddings with a Convolutional Layer
### 241. Exploring the Outputs of Our Convolutional Patch Embedding Layer
### 242. Flattening Our Convolutional Feature Maps into a Sequence of Patch Embeddings
### 243. Visualizing a Single Sequence Vector of Patch Embeddings
### 244. Creating the Patch Embedding Layer with PyTorch
### 245. Creating the Class Token Embedding
### 246. Creating the Class Token Embedding - Less Birds
### 247. Creating the Position Embedding
### 248. Equation 1: Putting it All Together
### 249. Equation 2: Multihead Attention Overview
### 250. Equation 2: Layernorm Overview
### 251. Turning Equation 2 into Code
### 252. Checking the Inputs and Outputs of Equation
### 253. Equation 3: Replication Overview
### 254. Turning Equation 3 into Code
### 255. Transformer Encoder Overview
### 256. Combining equation 2 and 3 to Create the Transformer Encoder
### 257. Creating a Transformer Encoder Layer with In-Built PyTorch Layer
### 258. Bringing Our Own Vision Transformer to Life - Part 1: Gathering the Pieces
### 259. Bringing Our Own Vision Transformer to Life - Part 2: The Forward Method
### 260. Getting a Visual Summary of Our Custom Vision Transformer
### 261. Creating a Loss Function and Optimizer from the ViT Paper
### 262. Training our Custom ViT on Food Vision Mini
### 263. Discussing what Our Training Setup Is Missing
### 264. Plotting a Loss Curve for Our ViT Model
### 265. Getting a Pretrained Vision Transformer from Torchvision and Setting it Up
### 266. Preparing Data to Be Used with a Pretrained ViT
### 267. Training a Pretrained ViT Feature Extractor Model for Food Vision Mini
### 268. Saving Our Pretrained ViT Model to File and Inspecting Its Size
### 269. Discussing the Trade-Offs Between Using a Larger Model for Deployments
### 270. Making Predictions on a Custom Image with Our Pretrained ViT
### 271. PyTorch Paper Replicating: Main Takeaways, Exercises and Extra-Curriculum

    7min
### 272. What is Machine Learning Model Deployment - Why Deploy a Machine Learning Model
### 273. Three Questions to Ask for Machine Learning Model Deployment
### 274. Where Is My Model Going to Go?
### 275. How Is My Model Going to Function?
### 276. Some Tools and Places to Deploy Machine Learning Models
### 277. What We Are Going to Cover
### 278. Getting Setup to Code
### 279. Downloading a Dataset for Food Vision Mini
### 280. Outlining Our Food Vision Mini Deployment Goals and Modelling Experiments
### 281. Creating an EffNetB2 Feature Extractor Model
### 282. Create a Function to Make an EffNetB2 Feature Extractor Model and Transforms
### 283. Creating DataLoaders for EffNetB2
### 284. Training Our EffNetB2 Feature Extractor and Inspecting the Loss Curves
### 285. Saving Our EffNetB2 Model to File
### 286. Getting the Size of Our EffNetB2 Model in Megabytes
### 287. Collecting Important Statistics and Performance Metrics for Our EffNetB2 Model
### 288. Creating a Vision Transformer Feature Extractor Model
### 289. Creating DataLoaders for Our ViT Feature Extractor Model
### 290. Training Our ViT Feature Extractor Model and Inspecting Its Loss Curves
### 291. Saving Our ViT Feature Extractor and Inspecting Its Size
### 292. Collecting Stats About Our-ViT Feature Extractor
### 293. Outlining the Steps for Making and Timing Predictions for Our Models
### 294. Creating a Function to Make and Time Predictions with Our Models
### 295. Making and Timing Predictions with EffNetB2
### 296. Making and Timing Predictions with ViT
### 297. Comparing EffNetB2 and ViT Model Statistics
### 298. Visualizing the Performance vs Speed Trade-off
### 299. Gradio Overview and Installation
### 300. Gradio Function Outline
### 301. Creating a Predict Function to Map Our Food Vision Mini Inputs to Outputs
### 302. Creating a List of Examples to Pass to Our Gradio Demo
### 303. Bringing Food Vision Mini to Life in a Live Web Application
### 304. Getting Ready to Deploy Our App Hugging Face Spaces Overview
### 305. Outlining the File Structure of Our Deployed App
### 306. Creating a Food Vision Mini Demo Directory to House Our App Files
### 307. Creating an Examples Directory with Example Food Vision Mini Images
### 308. Writing Code to Move Our Saved EffNetB2 Model File
### 309. Turning Our EffNetB2 Model Creation Function Into a Python Script
### 310. Turning Our Food Vision Mini Demo App Into a Python Script
### 311. Creating a Requirements File for Our Food Vision Mini App
### 312. Downloading Our Food Vision Mini App Files from Google Colab
### 313. Uploading Our Food Vision Mini App to Hugging Face Spaces Programmatically
### 314. Running Food Vision Mini on Hugging Face Spaces and Trying it Out
### 315. Food Vision Big Project Outline
### 316. Preparing an EffNetB2 Feature Extractor Model for Food Vision Big
### 317. Downloading the Food 101 Dataset
### 318. Creating a Function to Split Our Food 101 Dataset into Smaller Portions
### 319. Turning Our Food 101 Datasets into DataLoaders
### 320. Training Food Vision Big: Our Biggest Model Yet!
### 321. Outlining the File Structure for Our Food Vision Big
### 322. Downloading an Example Image and Moving Our Food Vision Big Model File
### 323. Saving Food 101 Class Names to a Text File and Reading them Back In
### 324. Turning Our EffNetB2 Feature Extractor Creation Function into a Python Script
### 325. Creating an App Script for Our Food Vision Big Model Gradio Demo
### 326. Zipping and Downloading Our Food Vision Big App Files
### 327. Deploying Food Vision Big to Hugging Face Spaces
### 328. PyTorch Mode Deployment: Main Takeaways, Extra-Curriculum and Exercises

    6min
### 329. Introduction to PyTorch 2.0
### 330. What We Are Going to Cover and PyTorch 2 Reference Materials
### 331. Getting Started with PyTorch 2 in Google Colab
### 332. PyTorch 2.0 - 30 Second Intro
### 333. Getting Setup for PyTorch 2
### 334. Getting Info from Our GPUs and Seeing if They're Capable of Using PyTorch 2
### 335. Setting the Default Device in PyTorch 2
### 336. Discussing the Experiments We Are Going to Run for PyTorch 2
### 337. Introduction to PyTorch 2
### 338. Creating a Function to Setup Our Model and Transforms
### 339. Discussing How to Get Better Relative Speedups for Training Models
### 340. Setting the Batch Size and Data Size Programmatically
### 341. Getting More Potential Speedups with TensorFloat-32
### 342. Downloading the CIFAR10 Dataset
### 343. Creating Training and Test DataLoaders
### 344. Preparing Training and Testing Loops with Timing Steps for PyTorch 2.0 timing
### 345. Experiment 1 - Single Run without torch.compile
### 346. Experiment 2 - Single Run with torch.compile
### 347. Comparing the Results of Experiment 1 and 2
### 348. Saving the Results of Experiment 1 and 2
### 349. Preparing Functions for Experiment 3 and 4
### 350. Experiment 3 - Training a Non-Compiled Model for Multiple Runs
### 351. Experiment 4 - Training a Compiled Model for Multiple Runs
### 352. Comparing the Results of Experiment 3 and 4
### 353. Potential Extensions and Resources to Learn More

    6min
### 354. Special Bonus Lecture

    1min
### 355. Thank You!
### 356. Become An Alumni
### 357. Endorsements on LinkedIn
### 358. Learning Guideline
